<!DOCTYPE html>
<html>
<head>
<title>capstone_report</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script>

<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js">
</script>
</head>
<body>
<p><style>
.fig {text-align:center; font-weight:bold;}
.table {font-weight:bold;}
img[alt=&quot;center&quot;] {display:block; margin:auto; text-align:center}
</style></p>
<p><style type="text/css">
.centerImg {text-align:center; display:block; margin:auto}
</style></p>
<h1>Machine Learning Engineer Nanodegree</h1>
<h2>Capstone Project</h2>
<h3>&quot;Development of a CNN model for Google Landmark Recognition Challenge using a reduced dataset&quot;</h3>
<p>Gap Kim<br />
June 15, 2018</p>
<h2>I. Definition</h2>
<h3>Project Overview</h3>
<p>Over the last years, deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in many fields such as visual, audio, medical, social, and sensor. In particular, object recognition has gained tremendous interest by engineers and scientists in artificial intelligence and computer vision. Deep learning allows computational models of multiple processing layers to learn and represent data with multiple levels of abstraction mimicking how the brain perceives and understands multimodal information, thus implicitly capturing intricate structures of large-scale data [<a href="https://www.hindawi.com/journals/cin/2018/7068349/">1</a>]. Among various methods developed in object recognition, Convolutional Neural Network (CNN) is of interest for this project. CNNs were inspired by the structure of a visual system and were shown to significantly outperform traditional machine learning approaches in computer vision and pattern recognition [<a href="https://scholar.google.com/scholar_lookup?title=Learning+deep+architectures+for+AI&amp;author=Y.+Bengio&amp;publication_year=2009">2</a>]. CNNs have been used in variety of fields, which includes but are not limited to object detection [<a href="https://ieeexplore.ieee.org/document/6909475/">3</a>], face recognition [<a href="https://ieeexplore.ieee.org/document/554195/">4</a>], and action/activity recognition [<a href="https://link.springer.com/article/10.1007%2Fs10618-017-0495-0">5</a>]. </p>
<p>In this project, a deep learning model based on CNNs is proposed for Google Landmark Recognition Challenge from Kaggle [<a href="https://www.kaggle.com/c/landmark-recognition-challenge">6</a>]. One of great obstacles to landmark recognition research is the lack of large annotated datasets. Hence Google Landmark Recognition Competition presents the largest worldwide dataset to date and challenges to build models that recognize the correct landmarks from the test images. A technology that can accurately predict landmark labels directly from image pixels can broadly benefit applications in various areas such as photo management, maps, aviation, and satellite images.</p>
<h3>Problem Statement</h3>
<p>The Landmark Recognition Challenge presents a dataset with a very large numbers of classes (~15,000 classes), but the number of training examples per class may not be very large. This makes the problem different from image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) where the aim is to recognize 1000 general object categories. The original Landmark Recognition Challenge dataset provides two files, train.csv and test.csv. The training set images each depict exactly one landmark. Test images may depict no landmark, one landmark, or more than one landmark. Each image has a unique id (a hash) and each landmark has a unique id (an integer). Due to restrictions on distributing the actual  image files, the dataset contains a url for each image. Due to computational limitation, however, a reduced dataset is proposed with 100 unique landmark ids in approximately 8,100 images. Stratified sampling is employed so that the ratios of images among the classes are preserved. </p>
<p>The CNN model will be built using the framework of TensorFlow with Keras library.  The CNN model will employ three basic neural layers, convolutional, pooling and fully connected layers. The convolutional layer utilizes various filters to generate feature maps. The pooling layer reduces the spatial dimensions of the input volume and helps to alleviate overfitting problems. The fully connected layer eventually converts 2D feature maps into 1D feature vector and is forwarded into total number of landmark ids for classification. </p>
<p>To improve the model accuracy and computational efficiency, regularization techniques such as dropout, batch normalization, and data augmentation has been considered when building the optimal CNN for the reduced dataset. The batch normalization technique [<a href="https://arxiv.org/abs/1502.03167">7</a>], which transforms mean activation close to 0 and the activation standard deviation close to 1, has shown to improve accuracy with fewer training steps in image recognition problems. Many times, landmark images may be rotated, take up only a portion of the whole image, and may be out of center. The data augmentation technique may help improve accuracy for such images. Moreover, data augmentation strategy can help with those landmark ids with only small number of training images. </p>
<h3>Evaluation Metric</h3>
<p>The accuracy is used as the overall evaluation metric when comparing the performance of the benchmark model and the solution model. The accuracy can be calculated by:</p>
<p>$$
\text{accuracy} = \frac{\text{Number of correctly predicted class}}{\text{Total number of predictions}}
$$</p>
<h2>II. Analysis</h2>
<h3>Data Exploration and Visualization</h3>
<p>The original number of images for training and test dataset are 1,225,029 and 117,703, respectively with a total of 14,951 unique landmark ids. Use of full dataset may require large storage capacity (in the order of hundred GB) and computational power not easily available to an individual. Therefore a subset of dataset will be used for this project as follows:</p>
<ol>
<li>First, top 100 landmark ids most frequently appearing are identified among 14,951 landmark ids.</li>
<li>Then, 2% of images from each of the 100 landmark ids are downloaded.</li>
</ol>
<p>The procedure is similar to stratified sampling applied to the top 100 frequent classes to preserve the ratio of images among the classes. The outlined procedure resulted in 8158 downloaded images with frequency distribution as shown in Fig. 1. It can be observed that some landmark ids have much higher frequency than others.</p>
<p><img src="Fig1.bmp" class="centerImg"></p>
<p class="fig"> Fig.1: Frequency distribution of 100 landmark ids in the dataset </p>
<p>An example of two images with the same landmark id is provided in Fig. 2.</p>
<img src="cc7258ede5a819d8.jpg" alt="drawing" width="400px"/>
<img src="dff13c78c1c70d62.jpg" alt="drawing" width="400px"/>
<p class="fig"> Fig.2: Two images with the same class (landmark_id=3065) </p>
<h3>Algorithms and Techniques</h3>
<h4>Base CNN</h4>
<p>CNNs are a category of neural networks that have been successfully used in image recognition and classification. A schematic of the structure of a CNN is shown in Fig. 3 with an example input image and output predictions. There are mainly three main neural layers: convolutional layer, pooling or subsampling layer, and fully connected layer. In the convolutional layer, multiple kernels are used to create feature maps for the input image. Each kernel produces a convolved image from the input image. A pooling layer typically follows a convolutional layer, which downsamples and reduces the dimensionality of the feature maps. This downsampling helps with overfitting issues and computational cost. The pooling layer also makes the network invariant to small transformations, distortions and translations in the input image and leads to almost scale invariant representation of the image [<a href="https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md">8</a>]. The convolutional and pooling layers can be repeated to form a deeper network. As the convolutional network becomes deeper, the feature map becomes smaller, more abstract and loses spatial information. These reduced feature maps are then fed into the fully connected layers where the high-level reasoning is performed. The neurons in the fully connected layer have full connections to all activation in the previous layer. The fully connected layer eventually converts 2D feature map to 1D feature vector and classifies them. In summary, the convolutional and pooling layers act as feature extractors from the input image while the fully connected layers act as a classifier.</p>
<p><img src="Fig3.jpg" alt="center" /></p>
<p class="fig"> Fig.3: A schematic of the CNN model  
<a href="https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f">[9] </a>
</p>
<h4>Data augmentation</h4>
<p>Data augmentation increases the training dataset by adding more images using existing images. New images can be created in various ways. For example, new images can be generated from the original image by random cropping, translation, rotation, reflection, distortion, scaling, etc. An example from [<a href="https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced">10</a>] shows an original image of a cat and newly created images through data augmentation.</p>
<p><img src="Fig4.jpg" class="centerImg" width="500"></p>
<p class="fig"> Fig.4: An example of images generated from data augmentation </p>
<p>As noted in Fig. 1, some landmark ids have only small amount of images. Data augmentation may help increase the prediction accuracy for those landmark ids with smaller amount of training data.</p>
<h4>Batch normalization</h4>
<p>Batch normalization involves normalization of the features across the training examples in each mini-batch. The layer first normalizes the activations of each channel by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. Then, the layer shifts the input by a learnable offset and scales it by a learnable scale factor [<a href="https://www.mathworks.com/help/nnet/ref/nnet.cnn.layer.batchnormalizationlayer.html">11</a>]. While there are competing explanations why batch normalization works, its effectiveness is apparent in various studies. Empirically it appears to stabilize the gradient (less exploding or vanishing values) and batch-normalized models appear to overfit less [<a href="https://gluon.mxnet.io/chapter04_convolutional-neural-networks/cnn-batch-norm-scratch.html">12</a>].</p>
<h3>Benchmark</h3>
<p>As shown in Fig. 1, the frequency of the 100 landmarks appearing in the dataset varies significantly. In other words, the expected accuracy of random guessing may vary depending on the selected test dataset. The accuracy of the benchmark model, given the test dataset, can be calculated as the following:</p>
<p>(i) Given the test dataset and 100 unique landmark ids, the probability of correctly classifying a landmark id, $P(id)$ can be calculated as,</p>
<p>$$
P(id) = n_{id} / N
$$</p>
<p>where $n_{id}$ is the number of the landmark id in the test dataset, and and $N$ is the total number of images in the test dataset. For example, if landmark id = 99 appears 50 times in the test dataset with size of 800, $P(99) = 50/800$.</p>
<p>(ii) The expected value of correct number of landmark id classified is:</p>
<p>$$
E(x) = \sum_{x=0}^{x=nid} (x \cdot P(id(x)))
$$</p>
<p>(iii) Finally, the expected accuracy for correctly classifying the landmark id is:</p>
<p>$$
E(accuracy) = \frac{E(x)}{N}
$$</p>
<p>The expected accuracy of the benchmark model as given above can be calculated for a given test dataset. Unfortunately, huge number of combinatorial calculation in step (ii) caused memory overflow and too long of a computational time. Hence a Monte Carlo simulation has been employed to estimate the expected accuracy. Random sampling size of $n_{id}$ landmark id was used, and simulations were run 10 times. Table 1 summarizes the expected correct number of E(x) and expected accuracy for 20 random selection from test dataset.</p>
<p class="table"> Table 1: Results of expected accuracy from Monte Carlo simulation </p>
<img src="Table1.jpg" alt="drawing" width="500px"/>
<p>Therefore, the mean number of correctly predicting landmark id is 0.791 out of 20 randomly selected images. Thus the mean expected accuracy is 3.96%. This means that random guessing of the landmark id will result in accuracy of approximately 4.0%.</p>
<h2>III. Methodology</h2>
<h3>Data Preprocessing</h3>
<p>The 8158 images have been split into training, validation, and test datasets with split ratio of 0.8:0.1:0.1 using stratified sampling method. Upon inspection of all images, however, some images were either corrupted or missing. Hence those images were manually removed from the respective dataset. Finally, the resulting training, validation, and test dataset sizes were 6480, 805, and 809 with 100 landmark ids in each of the dataset.</p>
<p>The training, validation, and test images are moved to respective directories using their image ids as filenames. Each image is loaded and converted to a 4D tensor (number of images, height, width, channels) that can be used for training, validating and testing in the Keras CNN model. The procedure is as follows:</p>
<ol>
<li>Load a RGB image into PIL image format with given target size of (192, 256)</li>
<li>Convert the PIL image to 3D tensor Numpy array of (192, 256, 3)</li>
<li>Convert the 3D tensor to a 4D tensor with shape (1, 192, 256, 3)</li>
<li>Stack all 4D tensors from all images into 4D tensor with shape (total number of images, 192, 256, 3) and rescale the pixel values by dividing them by 255</li>
</ol>
<h3>Implementation</h3>
<h4>Base CNN</h4>
<p>The Base CNN followed the sequence shown in Fig. 3. The base CNN only uses the three main types of layers: convolutional, pooling, and fully connected (dense) layers. The first convolutional layer consisted of 16 filters, and the filter numbers doubled as more convolutional layers were subsequently added to form deeper network. The kernel window size varied between 2 and 4, mostly using size of 3. All convolutional layers employed ReLU activation function with 'same' padding, which means that input and output length of the convolutional layer is the same. After each convolutional layer, a max pooling layer with size of 2 was used to prevent overfitting. </p>
<p>With the input shape of (192 256), the combination of convolutional and max pooling layers will result in output shapes of (96, 128), (48, 64), (24, 32), (12, 18) and (6, 9) as the network deepens. It seems reasonable to speculate that representative features would be at levels 3, 4 and 5 considering the size of the feature that can capture the characteristics of the landmark. Therefore, the combination of convolutional and max pooling layers were repeated 3 to 5 times to form a deeper network as a feature extractor. The features are then fed into series of fully connected layers with node numbers selected among 1024, 512, and 256. After each fully connected layer, a dropout layer with value of 0.3 was used. Finally, a fully connected layer with 100 nodes, which is the same as the total number of landmark ids, is used with the Softmax activation to predict the probabilities of each class (landmark id). The series of fully connected layers act as a classifier.</p>
<p>Total of 11 runs with various settings have been performed as summarized in Table 2. As an example, the CNN model for Run 1 is provided as follows:</p>
<ol>
<li>Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')</li>
<li>MaxPooling2D(pool_size=2)</li>
<li>Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')</li>
<li>MaxPooling2D(pool_size=2)</li>
<li>Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')</li>
<li>MaxPooling2D(pool_size=2)</li>
<li>Flatten()</li>
<li>Dense(512, activation='relu')</li>
<li>Dropout(0.3)</li>
<li>Dense(100, activation='softmax')</li>
</ol>
<p class="table"> Table 2: Summary of parameter settings used in Base CNN </p>
<table>
<thead>
<tr>
	<th align="center">Run</th>
	<th align="center">Accuracy</th>
	<th align="center">Filter</th>
	<th align="center">Kernel</th>
	<th align="center">Dense</th>
	<th align="center">Batch size</th>
</tr>
</thead>
<tbody>
<tr>
	<td align="center">1</td>
	<td align="center">0.2361</td>
	<td align="center">(16,32,64)</td>
	<td align="center">(3,3,3)</td>
	<td align="center">(512,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">2</td>
	<td align="center">0.3461</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(3,3,3,3)</td>
	<td align="center">(512,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">3</td>
	<td align="center">0.2917</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(3,3,3,3)</td>
	<td align="center">(512,100)</td>
	<td align="center">128</td>
</tr>
<tr>
	<td align="center">4</td>
	<td align="center">0.3004</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(3,3,3,3)</td>
	<td align="center">(512,100)</td>
	<td align="center">32</td>
</tr>
<tr>
	<td align="center">5</td>
	<td align="center">0.3337</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(4,4,3,3)</td>
	<td align="center">(512,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">6</td>
	<td align="center">0.3696</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(4,3,3,2)</td>
	<td align="center">(512,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">7</td>
	<td align="center">0.3523</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(4,3,3,2)</td>
	<td align="center">(1024,512,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">8</td>
	<td align="center">0.2930</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(4,3,3,2)</td>
	<td align="center">(256,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">9</td>
	<td align="center">0.3337</td>
	<td align="center">(16,32,64,128)</td>
	<td align="center">(4,3,3,2)</td>
	<td align="center">(512,256,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">10</td>
	<td align="center">0.3498</td>
	<td align="center">(16,32,64,128,256)</td>
	<td align="center">(3,3,3,3,3)</td>
	<td align="center">(512,100)</td>
	<td align="center">64</td>
</tr>
<tr>
	<td align="center">11**</td>
	<td align="center">0.4227</td>
	<td align="center">(16,32,64,128,256)</td>
	<td align="center">(4,3,3,2,2)</td>
	<td align="center">(1024,512,100)</td>
	<td align="center">64</td>
</tr>
</tbody>
</table>
<p>** Optimal setting</p>
<p>All other runs followed the same sequence with parameter settings given in Table 2. The test accuracy of the Base CNN ranged from 0.236 to 0.423, which was significantly higher than the benchmark model of random guessing (around 0.04). Looking at Runs 1, 2 and 10, it seems 4 to 5 convolutional-maxpooling layers are needed as Run 1 with 3 convolutional layers resulted in much lower test accuracy. Comparing Runs 2, 3 and 4, batch size of 64 gave the best test accuracy. Runs 2, 5 and 6 compare tweaking of kernel sizes, and decreasing the kernel for deeper network seems to give good results. Finally, Run 11 was selected as the best performing base CNN model with test accuracy of 0.423.</p>
<h4>Data Augmentation</h4>
<p>Data augmentation is performed using Keras ImageDataGenerator function as shown in the code block below. Selected parameters are rotation_range, width_shift_range, height_shift_range, and zoom_range. </p>
<pre><code>from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rotation_range=45, 
    width_shift_range=0.1, 
    height_shift_range=0.1, 
    zoom_range=0.3)

valid_datagen = ImageDataGenerator()

train_generator = train_datagen.flow(train_tensors, train_target, batch_size= 128)
valid_generator = valid_datagen.flow(valid_tensors, valid_target, batch_size= 128)
</code></pre>

<p>Using the best performing base CNN model (Run 11 from Table 2), different parameter combinations have been tested as shown in Table 3. Due to extensive running time, requiring epochs over 50, only three settings have been tested. RUn 1 resulted in highest test accuracy of 0.666.</p>
<p class="table"> Table 3: Summary of parameter settings used in Data Augmentation </p>
<table>
<thead>
<tr>
	<th align="center">Run</th>
	<th align="center">Accuracy</th>
	<th align="center">Rot_angle</th>
	<th align="center">width_shift</th>
	<th align="center">height_shift</th>
	<th align="center">zoom</th>
</tr>
</thead>
<tbody>
<tr>
	<td align="center">1**</td>
	<td align="center">0.6663</td>
	<td align="center">10</td>
	<td align="center">0.2</td>
	<td align="center">0.2</td>
	<td align="center">0.4</td>
</tr>
<tr>
	<td align="center">2</td>
	<td align="center">0.6267</td>
	<td align="center">45</td>
	<td align="center">0.2</td>
	<td align="center">0.2</td>
	<td align="center">0.3</td>
</tr>
<tr>
	<td align="center">3</td>
	<td align="center">0.5290</td>
	<td align="center">90</td>
	<td align="center">0.2</td>
	<td align="center">0.2</td>
	<td align="center">0.3</td>
</tr>
</tbody>
</table>
<p>** Optimal setting</p>
<h4>Batch Normalization</h4>
<p>There are abundant discussions as to how batch normalization needs to be implemented in a convolutional layer. Some scientists suggest that batch normalization layer should be used before the activation layer as in the original paper by Loffe and Szegedy [<a href="https://arxiv.org/abs/1502.03167">13</a>] and as noted in various web blogs [<a href="https://gluon.mxnet.io/chapter04_convolutional-neural-networks/cnn-batch-norm-scratch.html">14</a>] [<a href="https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/">15</a>]. An example code block is provided below:</p>
<pre><code>modelBN.add(Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False))
modelBN.add(BatchNormalization())
modelBN.add(Activation(&quot;relu&quot;))
modelBN.add(MaxPooling2D(pool_size=2))
</code></pre>

<p>On the other hand, some recommend using the batch normalization layer after the activation layer [<a href="https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md">16</a>] as:</p>
<pre><code>modelBN.add(Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False))
modelBN.add(Activation(&quot;relu&quot;))
modelBN.add(BatchNormalization())
modelBN.add(MaxPooling2D(pool_size=2))
</code></pre>

<p>The batch normalization layer has been implemented both ways using the setting from Run 7 in Table 2. Unfortunately, the batch normalization did not particularly increase the </p>
<p>Unfortunately, the test accuracy was very poor with batch normalization. Test accuracy was 0.1236 and 0.1298 with batch normalization before activation layer and after activation layer, respectively. The results in Table 2 were obtained from using Adam as the optimizer with default settings. The accuracy with batch normalization improved with trying different learning rates and trying different adaptive optimizers, but required longer training time with more epochs. For the dataset given, the batch normalization was not helpful in reducing time or improving accuracy, and therefore, was not included in the final model.</p>
<h2>IV. Results</h2>
<h3>Model Evaluation and Validation</h3>
<p>A schematic of the final model selected (Run 11 in Table 2) is shown in Fig. 5. The model had total of 13,348,180 parameters. The convolutional and max pooling layers had five repeating structures where the image size of (192,256) reduced to feature extraction of (6,8). The extracted features then underwent three fully dense layers for classification of the image. </p>
<p><img src="Fig5.png" class="centerImg" width="500px"></p>
<p class="fig"> Fig.5: Final model used in prediction of Landmark </p>
<p>The Base CNN had test accuracy of 0.4227, and the accuracy and loss functions are plotted in Fig.6. It can be seen that overfitting occurs after epoch 6 where the validation accuracy starts to decrease and validation loss increases.</p>
<p><img src="base.png" class="centerImg"></p>
<p class="fig"> Fig.6: Accuracy and loss function of training and validation dataset of the Base CNN model </p>
<p>The final model incorporated a data augmentation scheme with settings given in Run 1 of Table 3. The test accuracy increased from 0.4227 to 0.6663. The accuracy and loss function of the training and validation dataset is provided in Fig. 7. While the training loss function continually decrease, the validation loss function flattens after epoch 40. </p>
<p><img src="aug.png" class="centerImg"></p>
<p class="fig"> Fig.7: Accuracy and loss function of training and validation dataset of the Base CNN model with data augmentation </p>
<h3>Justification</h3>
<p>The benchmark metric as described earlier was the probability of randomly guessing the correct landmark id, which was only about 4%. It is very obvious the CNN model with data augmentation outperformed the random guessing by a wide margin.</p>
<p>The Global Average Precision (GAP) at <em>k</em>=1, which was used as the evaluation metric in the Google Landmark Recognition Challenge, has been calculated as a reference. The GAP metric as defined in the competition is the following:</p>
<p>$$
GAP = \frac{1}{M} \sum_{i=1}^{N}P(i)rel(i)
$$ </p>
<p>where:</p>
<ul>
<li><em>N</em> is the total number of predictions returned by the system, across all queries</li>
<li><em>M</em> is the total number of queries with at least one landmark from the training set visible in it (note that some queries may not depict landmarks)</li>
<li><em>P(i)</em> is the precision at rank <em>i</em></li>
<li><em>rel(i)</em> denotes the relevance of prediction <em>i</em>: it’s 1 if the <em>i</em>-th prediction is correct, and 0 otherwise</li>
</ul>
<p>However, it must be noted that a direct comparison between the scores from the competition and the project may be inappropriate. The images used in this project is only about 2% of the training dataset. Moreover, the original test dataset consists of landmark ids not in the training dataset. Thus <em>M</em> is greater than <em>N</em> in the original competition. By design of this project, however, all landmark ids in the test dataset are included in the training dataset. Therefore, <em>M</em> equals <em>N</em> in this project, and the GAP score from this project is expected to be much higher than those of competition scores. Nonetheless, the scores from competition may be used as a reference score. The top three final leaderboard scores from the competition were 0.304, 0.290 and 0.289 as reported from the competition website [<a href="https://www.kaggle.com/c/landmark-recognition-challenge/leaderboard">17</a>]. The GAP score calculated from the final model with data augmentation is 89%, which is much higher than those from the competition. </p>
<h2>V. Conclusion</h2>
<h3>Free-Form Visualization</h3>
<p>A schematic of the final CNN model developed is provided in Fig. 8. The input image with shape of (192, 256) is supplied to the image extraction network, which consists of five sequential combination of convolutional and max pooling layers. The extracted features are supplied to three fully connected layers with dropout layer to identify 100 landmark ids. The accuracy of final CNN with data augmentation was 66.6%, which was significantly higher than random guession of CNN without data augmentation.</p>
<p><img src="Images_for_ppt.png" class="centerImg"></p>
<p class="fig"> Fig.8: Final CNN model and performance </p>
<h3>Reflection</h3>
<p>In this project, a deep learning model based on CNNs is proposed for Google Landmark Recognition Challenge from Kaggle. The original dataset from Google Landmark Recognition Challenge has been reduced for this project due to computational limitation. First, top 100 landmark ids most frequently appearing were identified among 14,951 landmark ids. Then, 2% of images from each of the 100 landmark ids were downloaded. The procedure is similar to stratified sampling applied to the top 100 frequent classes. Among the 8158 downloaded images, the dataset was split into training, validation, and test datasets with split ratio of 0.8:0.1:0.1 using stratified sampling.</p>
<p>The Base CNN consisted of three main types of layers: convolutional, pooling, and fully connected (dense) layers. The convolutional layer was followed by the max pooling layer, which acted as a feature extraction layer. The combination of convolutional-max pooling layer were repeated to form a deep network. As the network deepens the relevant features are identified while the spatial information is lost. The extracted features are supplied to series of fully connected layers, which function as a classifier. The last fully connected layer employed Softmax function to predict the probability of each landmark id (class).</p>
<p>Total of 11 different settings were tested. The best performing Base CNN had five series of convolutional-max pooling layer that identified feature of shape (6, 8) pixels from original image shape of (192, 256) pixels. The features then underwent three fully connected layers to predict the probability of each landmark id. The best performing Base CNN resulted in accuracy of 42.3%.</p>
<p>The data augmentation technique was employed as it can effectively increase the input data without overfitting issues. Since the same landmark image may shift, rotate or zoom differently in various images, the technique can particularly help with improving accuracies of those landmark ids with small number of training images. As a result, the accuracy significantly improved to 66.6%.</p>
<h3>Improvement</h3>
<p>There may be several ways to improve the model. One method is to take advantage of transfer learning. Pre-trained models that can identify humans, trees, animals (like pets), and vehicles are available. The landmark images often contain these objects since many photos are taken by tourists, which include people and natural surroundings. Excluding these objects from the images may help with training and identifying the relevant features to extract.</p>
<p>Another method may be to use a different classifier in developing the model. Instead of using the fully connected layers as the classifier, other classifiers such as support vector machines, variants of decision trees (with boosting or random forest), and nearest neighbor may be used after the feature extraction layer. </p>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
